{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-18T01:06:05.199959Z","iopub.status.busy":"2022-11-18T01:06:05.199512Z","iopub.status.idle":"2022-11-18T01:06:05.227125Z","shell.execute_reply":"2022-11-18T01:06:05.225922Z","shell.execute_reply.started":"2022-11-18T01:06:05.199854Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import string\n","import os\n","from PIL import Image\n","import glob\n","import pickle\n","from time import time"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:05.229671Z","iopub.status.busy":"2022-11-18T01:06:05.229442Z","iopub.status.idle":"2022-11-18T01:06:05.246355Z","shell.execute_reply":"2022-11-18T01:06:05.244820Z","shell.execute_reply.started":"2022-11-18T01:06:05.229648Z"},"trusted":true},"outputs":[],"source":["# load descriptions\n","def load_doc(filename):\n","    file = open(filename, 'r')\n","    text = file.read()\n","    file.close()\n","    return text\n","    \n","  \n","def load_descriptions(doc):\n","    mapping = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        if len(line) < 2:\n","            continue\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        image_id = image_id.split('.')[0]\n","        image_desc = ' '.join(image_desc)\n","        if image_id not in mapping:\n","            mapping[image_id] = list()\n","        mapping[image_id].append(image_desc)\n","    return mapping\n","  \n","def clean_descriptions(descriptions):\n","    table = str.maketrans('', '', string.punctuation)\n","    for key, desc_list in descriptions.items():\n","        for i in range(len(desc_list)):\n","            desc = desc_list[i]\n","            desc = desc.split()\n","            desc = [word.lower() for word in desc]\n","            desc = [w.translate(table) for w in desc]\n","            desc = [word for word in desc if len(word)>1]\n","            desc = [word for word in desc if word.isalpha()]\n","            desc_list[i] =  ' '.join(desc)\n","            \n","    return descriptions\n","\n","# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","    lines = list()\n","    for key, desc_list in descriptions.items():\n","        for desc in desc_list:\n","            lines.append(key + ' ' + desc)\n","    data = '\\n'.join(lines)\n","    file = open(filename, 'w')\n","    file.write(data)\n","    file.close()\n","\n","\n","# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","    doc = load_doc(filename)\n","    descriptions = dict()\n","    for line in doc.split('\\n'):\n","        tokens = line.split()\n","        image_id, image_desc = tokens[0], tokens[1:]\n","        if image_id in dataset:\n","            if image_id not in descriptions:\n","                descriptions[image_id] = list()\n","            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","            descriptions[image_id].append(desc)\n","    return descriptions\n","  \n","def load_set(filename):\n","    doc = load_doc(filename)\n","    dataset = list()\n","    for line in doc.split('\\n'):\n","        if len(line) < 1:\n","            continue\n","        identifier = line.split('.')[0]\n","        dataset.append(identifier)\n","    return set(dataset)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:05.249379Z","iopub.status.busy":"2022-11-18T01:06:05.248080Z","iopub.status.idle":"2022-11-18T01:06:05.963143Z","shell.execute_reply":"2022-11-18T01:06:05.962224Z","shell.execute_reply.started":"2022-11-18T01:06:05.249319Z"},"trusted":true},"outputs":[],"source":["filename = \"../input/flickr8k/Flickr8k_text/Flickr8k.token.txt\"\n","doc = load_doc(filename)\n","descriptions = load_descriptions(doc)\n","descriptions = clean_descriptions(descriptions)\n","save_descriptions(descriptions, 'descriptions.txt')\n","filename = '../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","train_descriptions = load_clean_descriptions('descriptions.txt', train)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:05.966280Z","iopub.status.busy":"2022-11-18T01:06:05.965990Z","iopub.status.idle":"2022-11-18T01:06:26.393499Z","shell.execute_reply":"2022-11-18T01:06:26.392400Z","shell.execute_reply.started":"2022-11-18T01:06:05.966257Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Preprocessed words 7578 -> 1651\n"]}],"source":["# Create a list of all the training captions\n","all_train_captions = []\n","for key, val in train_descriptions.items():\n","    for cap in val:\n","        all_train_captions.append(cap)\n","        \n","        \n","# Consider only words which occur at least 10 times in the corpus\n","word_count_threshold = 10\n","word_counts = {}\n","nsents = 0\n","for sent in all_train_captions:\n","    nsents += 1\n","    for w in sent.split(' '):\n","        word_counts[w] = word_counts.get(w, 0) + 1\n","\n","vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n","print('Preprocessed words {} -> {}'.format(len(word_counts), len(vocab)))\n","\n","\n","ixtoword = {}\n","wordtoix = {}\n","\n","ix = 1\n","for w in vocab:\n","    wordtoix[w] = ix\n","    ixtoword[ix] = w\n","    ix += 1\n","    \n","vocab_size = len(ixtoword) + 1 # one for appended 0's\n","\n","# Load Glove vectors\n","glove_dir = 'glove.6B'\n","embeddings_index = {}\n","f = open('../input/glove6b200d/glove.6B.200d.txt', encoding=\"utf-8\")\n","\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","embedding_dim = 200\n","\n","# Get 200-dim dense vector for each of the words in out vocabulary\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in wordtoix.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:26.395635Z","iopub.status.busy":"2022-11-18T01:06:26.395268Z","iopub.status.idle":"2022-11-18T01:06:27.164101Z","shell.execute_reply":"2022-11-18T01:06:27.163423Z","shell.execute_reply.started":"2022-11-18T01:06:26.395600Z"},"trusted":true},"outputs":[],"source":["\n","# # Below path contains all the images\n","# all_images_path = '../input/flickr8k/Flickr8k_Dataset/'\n","# # Create a list of all image names in the directory\n","# all_images = glob.glob(all_images_path + '*.jpg')\n","\n","# # Create a list of all the training and testing images with their full path names\n","# def create_list_of_images(file_path):\n","#     images_names = set(open(file_path, 'r').read().strip().split('\\n'))\n","#     images = []\n","\n","#     for image in all_images: \n","#         if image[len(all_images_path):] in images_names:\n","#             images.append(image)\n","  \n","#     return images\n","\n","\n","# train_images_path = '../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n","# test_images_path = '../input/flickr8k/Flickr8k_text/Flickr_8k.testImages.txt'\n","\n","# train_images = create_list_of_images(train_images_path)\n","# test_images = create_list_of_images(test_images_path)\n","\n","# #preprocessing the images\n","# def preprocess(image_path):\n","#     img = image.load_img(image_path, target_size=(299, 299))\n","#     x = image.img_to_array(img)\n","#     x = np.expand_dims(x, axis=0)\n","#     x = preprocess_input(x)\n","#     return x\n","\n","# # Load the inception v3 model\n","# model = InceptionV3(weights='imagenet')\n","\n","# # Create a new model, by removing the last layer (output layer) from the inception v3\n","# model_new = Model(model.input, model.layers[-2].output)\n","\n","# # Encoding a given image into a vector of size (2048, )\n","# def encode(image):\n","#     image = preprocess(image) \n","#     fea_vec = model_new.predict(image) \n","#     fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n","#     return fea_vec\n","  \n","\n","# encoding_train = {}\n","# for img in train_images:\n","#     encoding_train[img[len(all_images_path):]] = encode(img)\n","    \n","    \n","# encoding_test = {}\n","# for img in test_images:\n","#     encoding_test[img[len(all_images_path):]] = encode(img)\n","    \n","# #Save the bottleneck features to disk\n","# with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n","#     pickle.dump(encoding_train, encoded_pickle)\n","    \n","# with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n","#     pickle.dump(encoding_test, encoded_pickle)\n","    \n","    \n","train_features = open(\"../input/image-caption-dataset/encoded_train_images.pkl\", \"rb\")\n","train_features = pickle.load(train_features)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:27.165532Z","iopub.status.busy":"2022-11-18T01:06:27.165167Z","iopub.status.idle":"2022-11-18T01:06:27.266878Z","shell.execute_reply":"2022-11-18T01:06:27.265872Z","shell.execute_reply.started":"2022-11-18T01:06:27.165507Z"},"trusted":true},"outputs":[],"source":["from tqdm.notebook import tqdm"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:27.268437Z","iopub.status.busy":"2022-11-18T01:06:27.268106Z","iopub.status.idle":"2022-11-18T01:06:28.282587Z","shell.execute_reply":"2022-11-18T01:06:28.281992Z","shell.execute_reply.started":"2022-11-18T01:06:27.268406Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21784ac77f7342e8ab2d0b3edcfff954","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/40460 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["list_to_csv = []\n","with open('../input/image-caption-dataset/descriptions.txt', 'r') as descriptions:\n","    lines = descriptions.readlines()\n","    for index in tqdm(range(0, len(lines))):\n","        image_and_words = lines[index].replace('\\n', '').split(' ')\n","        \n","        image = image_and_words[0]\n","        words = image_and_words[1:]\n","        \n","        new_list_word = [word for word in words if word in wordtoix]\n","        if len(new_list_word) == 0:\n","            continue\n","        \n","        list_to_csv.append([image, 'startseq', new_list_word[0]])\n","        \n","        phrase_to_each_row = ['sartseq']\n","        for index_word, word in enumerate(new_list_word):\n","            if index_word == (len(new_list_word) - 1):\n","                phrase_to_each_row.append(word)\n","                list_to_csv.append([image, ' '.join(phrase_to_each_row), 'endseq'])\n","            else:\n","                phrase_to_each_row.append(word)\n","                list_to_csv.append([image, ' '.join(phrase_to_each_row), new_list_word[index_word + 1]])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:28.284231Z","iopub.status.busy":"2022-11-18T01:06:28.283645Z","iopub.status.idle":"2022-11-18T01:06:28.292490Z","shell.execute_reply":"2022-11-18T01:06:28.291702Z","shell.execute_reply.started":"2022-11-18T01:06:28.284208Z"},"trusted":true},"outputs":[],"source":["Ã­magens = []\n","with open('../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt', 'r') as train_images:\n","    images = train_images.readlines()\n","    imagens = [image.replace('.jpg\\n', '') for image in images]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:28.294035Z","iopub.status.busy":"2022-11-18T01:06:28.293810Z","iopub.status.idle":"2022-11-18T01:06:29.142343Z","shell.execute_reply":"2022-11-18T01:06:29.141375Z","shell.execute_reply.started":"2022-11-18T01:06:28.294012Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image</th>\n","      <th>text_input</th>\n","      <th>word_output</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000268201_693b08cb0e</td>\n","      <td>startseq</td>\n","      <td>child</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000268201_693b08cb0e</td>\n","      <td>sartseq child</td>\n","      <td>in</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000268201_693b08cb0e</td>\n","      <td>sartseq child in</td>\n","      <td>pink</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000268201_693b08cb0e</td>\n","      <td>sartseq child in pink</td>\n","      <td>dress</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000268201_693b08cb0e</td>\n","      <td>sartseq child in pink dress</td>\n","      <td>is</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   image                   text_input word_output\n","0  1000268201_693b08cb0e                     startseq       child\n","1  1000268201_693b08cb0e                sartseq child          in\n","2  1000268201_693b08cb0e             sartseq child in        pink\n","3  1000268201_693b08cb0e        sartseq child in pink       dress\n","4  1000268201_693b08cb0e  sartseq child in pink dress          is"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data_frame_image_words = pd.DataFrame(list_to_csv, columns=['image', 'text_input', 'word_output'])\n","data_frame_image_words.to_csv('image_descriptions.csv', index=False)\n","data_frame_image_words.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:29.146438Z","iopub.status.busy":"2022-11-18T01:06:29.146194Z","iopub.status.idle":"2022-11-18T01:06:29.227135Z","shell.execute_reply":"2022-11-18T01:06:29.225270Z","shell.execute_reply.started":"2022-11-18T01:06:29.146417Z"},"trusted":true},"outputs":[],"source":["train_data_frame = data_frame_image_words.loc[data_frame_image_words['image'].isin(imagens)].reset_index(drop=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:29.230723Z","iopub.status.busy":"2022-11-18T01:06:29.230423Z","iopub.status.idle":"2022-11-18T01:06:30.763095Z","shell.execute_reply":"2022-11-18T01:06:30.761778Z","shell.execute_reply.started":"2022-11-18T01:06:29.230698Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import Adam, SGD, AdamW\n","from torch.nn.utils.rnn import pad_sequence\n","import random, os"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:07:00.414867Z","iopub.status.busy":"2022-11-18T01:07:00.414563Z","iopub.status.idle":"2022-11-18T01:07:00.419873Z","shell.execute_reply":"2022-11-18T01:07:00.418788Z","shell.execute_reply.started":"2022-11-18T01:07:00.414843Z"},"trusted":true},"outputs":[],"source":["CFG = {\n","    'LR': 1e-3,\n","    'EPOCHS': 5,\n","    'BATCH_SIZE': 512,\n","    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n","}"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:30.772881Z","iopub.status.busy":"2022-11-18T01:06:30.772610Z","iopub.status.idle":"2022-11-18T01:06:30.785032Z","shell.execute_reply":"2022-11-18T01:06:30.784079Z","shell.execute_reply.started":"2022-11-18T01:06:30.772821Z"},"trusted":true},"outputs":[],"source":["class CustomDataLoader(torch.utils.data.Dataset):\n","    def __init__(self, data_frame= None, word_to_index = None, images_dict = None):\n","        self.data_frame = data_frame\n","        self.images_dict = images_dict\n","        self.word_to_index = word_to_index\n","        self.pad_tensor = torch.ones(34)\n","        self.num_classes = len(word_to_index)\n","    \n","    def __len__(self):\n","        return len(self.data_frame)\n","    \n","    def to_categorical(self, y):\n","    \n","        return np.eye(self.num_classes + 1)[y]\n","    \n","    def __getitem__(self, index):\n","        image_phrase_word = self.data_frame.iloc[index].values\n","        \n","        image_name = image_phrase_word[0]\n","        phrase = image_phrase_word[1]\n","        word_to_predict = image_phrase_word[2]\n","        \n","        phrase_to_index = [self.word_to_index[word] for word in phrase.split(' ') if word in self.word_to_index]\n","        phrase_to_index = torch.FloatTensor(phrase_to_index)\n","        \n","        word_to_predict_index = self.word_to_index[word_to_predict]\n","        \n","        image = self.images_dict[image_name + '.jpg']\n","        \n","        image = torch.tensor(image, dtype=torch.float32)\n","        phrase_indexs = torch.tensor(pad_sequence([phrase_to_index, self.pad_tensor], batch_first=True)[0], dtype=torch.long)\n","        #target = torch.tensor(word_to_predict_index, dtype=torch.long)\n","        target = torch.tensor(torch.from_numpy(self.to_categorical(word_to_predict_index)), dtype=torch.float32)\n","\n","        return image, phrase_indexs, target"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:07:03.444462Z","iopub.status.busy":"2022-11-18T01:07:03.444139Z","iopub.status.idle":"2022-11-18T01:07:03.451176Z","shell.execute_reply":"2022-11-18T01:07:03.450023Z","shell.execute_reply.started":"2022-11-18T01:07:03.444438Z"},"trusted":true},"outputs":[],"source":["train = CustomDataLoader(data_frame = train_data_frame, word_to_index = wordtoix, images_dict = train_features)\n","\n","train_loader = torch.utils.data.DataLoader(train,\n","                                        shuffle=True,\n","                                        pin_memory=True,\n","                                        batch_size=CFG['BATCH_SIZE'],\n","                                        num_workers=0)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:30.803436Z","iopub.status.busy":"2022-11-18T01:06:30.802043Z","iopub.status.idle":"2022-11-18T01:06:30.813702Z","shell.execute_reply":"2022-11-18T01:06:30.812504Z","shell.execute_reply.started":"2022-11-18T01:06:30.803410Z"},"trusted":true},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.dp_05 = nn.Dropout(p = 0.5)\n","        self.dp_05_2 = nn.Dropout(p = 0.5)\n","        self.linear_image_input = nn.Linear(2048, 256)\n","        self.linear_output = nn.Linear(512, num_classes + 1)\n","        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix).type(torch.float32))\n","        self.rnn = nn.LSTM(200, 256, batch_first=True)\n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, image_features, phrase):\n","        image_features = self.dp_05(image_features)\n","        image_features = self.linear_image_input(image_features)\n","        image_features = self.relu(image_features)\n","        \n","        phrase_embedidng = self.embedding(phrase)\n","        phrase_embedidng = self.dp_05_2(phrase_embedidng)\n","        phrase_rnn = self.rnn(phrase_embedidng)[0]\n","        image_features_and_phrase = torch.cat((image_features, phrase_rnn[:, -1, :]), 1)\n","        output = self.linear_output(image_features_and_phrase)\n","        \n","        return output"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-11-18T01:06:30.815926Z","iopub.status.busy":"2022-11-18T01:06:30.815161Z","iopub.status.idle":"2022-11-18T01:06:30.846456Z","shell.execute_reply":"2022-11-18T01:06:30.845782Z","shell.execute_reply.started":"2022-11-18T01:06:30.815891Z"},"trusted":true},"outputs":[],"source":["model = CustomModel(len(wordtoix)).to(CFG['DEVICE'])\n","_loss = nn.CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=CFG['LR'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["loss_mean = []\n","\n","see_loss = 40\n","\n","for epoch in range(CFG['EPOCHS']):\n","    for i, (image, phrase, target) in enumerate(tqdm(train_loader, total=len(train_loader))):\n","        \n","        image = image.to(CFG['DEVICE'])\n","        target = target.to(CFG['DEVICE'])\n","        phrase = phrase.to(CFG['DEVICE'])\n","        \n","        output = model(image, phrase)\n","        \n","        optimizer.zero_grad()\n","        loss = _loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        loss_mean.append(loss.item())\n","        \n","        if (i % see_loss) == 0:\n","            print(np.mean(loss_mean))\n","            loss_mean = []\n","        \n","        del image, phrase, target, output\n","    "]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
