{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom numpy import array\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string\nimport os\nfrom PIL import Image\nimport glob\nimport pickle\nfrom time import time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-18T01:06:05.199512Z","iopub.execute_input":"2022-11-18T01:06:05.199959Z","iopub.status.idle":"2022-11-18T01:06:05.227125Z","shell.execute_reply.started":"2022-11-18T01:06:05.199854Z","shell.execute_reply":"2022-11-18T01:06:05.225922Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# load descriptions\ndef load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text\n    \n  \ndef load_descriptions(doc):\n    mapping = dict()\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n        image_id, image_desc = tokens[0], tokens[1:]\n        image_id = image_id.split('.')[0]\n        image_desc = ' '.join(image_desc)\n        if image_id not in mapping:\n            mapping[image_id] = list()\n        mapping[image_id].append(image_desc)\n    return mapping\n  \ndef clean_descriptions(descriptions):\n    table = str.maketrans('', '', string.punctuation)\n    for key, desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n            desc = desc.split()\n            desc = [word.lower() for word in desc]\n            desc = [w.translate(table) for w in desc]\n            desc = [word for word in desc if len(word)>1]\n            desc = [word for word in desc if word.isalpha()]\n            desc_list[i] =  ' '.join(desc)\n            \n    return descriptions\n\n# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        image_id, image_desc = tokens[0], tokens[1:]\n        if image_id in dataset:\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            descriptions[image_id].append(desc)\n    return descriptions\n  \ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    for line in doc.split('\\n'):\n        if len(line) < 1:\n            continue\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:05.229442Z","iopub.execute_input":"2022-11-18T01:06:05.229671Z","iopub.status.idle":"2022-11-18T01:06:05.246355Z","shell.execute_reply.started":"2022-11-18T01:06:05.229648Z","shell.execute_reply":"2022-11-18T01:06:05.244820Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"filename = \"../input/flickr8k/Flickr8k_text/Flickr8k.token.txt\"\ndoc = load_doc(filename)\ndescriptions = load_descriptions(doc)\ndescriptions = clean_descriptions(descriptions)\nsave_descriptions(descriptions, 'descriptions.txt')\nfilename = '../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:05.248080Z","iopub.execute_input":"2022-11-18T01:06:05.249379Z","iopub.status.idle":"2022-11-18T01:06:05.963143Z","shell.execute_reply.started":"2022-11-18T01:06:05.249319Z","shell.execute_reply":"2022-11-18T01:06:05.962224Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Create a list of all the training captions\nall_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)\n        \n        \n# Consider only words which occur at least 10 times in the corpus\nword_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\nprint('Preprocessed words {} -> {}'.format(len(word_counts), len(vocab)))\n\n\nixtoword = {}\nwordtoix = {}\n\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1\n    \nvocab_size = len(ixtoword) + 1 # one for appended 0's\n\n# Load Glove vectors\nglove_dir = 'glove.6B'\nembeddings_index = {}\nf = open('../input/glove6b200d/glove.6B.200d.txt', encoding=\"utf-8\")\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nembedding_dim = 200\n\n# Get 200-dim dense vector for each of the words in out vocabulary\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:05.965990Z","iopub.execute_input":"2022-11-18T01:06:05.966280Z","iopub.status.idle":"2022-11-18T01:06:26.393499Z","shell.execute_reply.started":"2022-11-18T01:06:05.966257Z","shell.execute_reply":"2022-11-18T01:06:26.392400Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Preprocessed words 7578 -> 1651\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# # Below path contains all the images\n# all_images_path = '../input/flickr8k/Flickr8k_Dataset/'\n# # Create a list of all image names in the directory\n# all_images = glob.glob(all_images_path + '*.jpg')\n\n# # Create a list of all the training and testing images with their full path names\n# def create_list_of_images(file_path):\n#     images_names = set(open(file_path, 'r').read().strip().split('\\n'))\n#     images = []\n\n#     for image in all_images: \n#         if image[len(all_images_path):] in images_names:\n#             images.append(image)\n  \n#     return images\n\n\n# train_images_path = '../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt'\n# test_images_path = '../input/flickr8k/Flickr8k_text/Flickr_8k.testImages.txt'\n\n# train_images = create_list_of_images(train_images_path)\n# test_images = create_list_of_images(test_images_path)\n\n# #preprocessing the images\n# def preprocess(image_path):\n#     img = image.load_img(image_path, target_size=(299, 299))\n#     x = image.img_to_array(img)\n#     x = np.expand_dims(x, axis=0)\n#     x = preprocess_input(x)\n#     return x\n\n# # Load the inception v3 model\n# model = InceptionV3(weights='imagenet')\n\n# # Create a new model, by removing the last layer (output layer) from the inception v3\n# model_new = Model(model.input, model.layers[-2].output)\n\n# # Encoding a given image into a vector of size (2048, )\n# def encode(image):\n#     image = preprocess(image) \n#     fea_vec = model_new.predict(image) \n#     fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n#     return fea_vec\n  \n\n# encoding_train = {}\n# for img in train_images:\n#     encoding_train[img[len(all_images_path):]] = encode(img)\n    \n    \n# encoding_test = {}\n# for img in test_images:\n#     encoding_test[img[len(all_images_path):]] = encode(img)\n    \n# #Save the bottleneck features to disk\n# with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n#     pickle.dump(encoding_train, encoded_pickle)\n    \n# with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n#     pickle.dump(encoding_test, encoded_pickle)\n    \n    \ntrain_features = open(\"../input/image-caption-dataset/encoded_train_images.pkl\", \"rb\")\ntrain_features = pickle.load(train_features)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:26.395268Z","iopub.execute_input":"2022-11-18T01:06:26.395635Z","iopub.status.idle":"2022-11-18T01:06:27.164101Z","shell.execute_reply.started":"2022-11-18T01:06:26.395600Z","shell.execute_reply":"2022-11-18T01:06:27.163423Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:27.165167Z","iopub.execute_input":"2022-11-18T01:06:27.165532Z","iopub.status.idle":"2022-11-18T01:06:27.266878Z","shell.execute_reply.started":"2022-11-18T01:06:27.165507Z","shell.execute_reply":"2022-11-18T01:06:27.265872Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"list_to_csv = []\nwith open('../input/image-caption-dataset/descriptions.txt', 'r') as descriptions:\n    lines = descriptions.readlines()\n    for index in tqdm(range(0, len(lines))):\n        image_and_words = lines[index].replace('\\n', '').split(' ')\n        \n        image = image_and_words[0]\n        words = image_and_words[1:]\n        \n        new_list_word = [word for word in words if word in wordtoix]\n        if len(new_list_word) == 0:\n            continue\n        \n        list_to_csv.append([image, 'startseq', new_list_word[0]])\n        \n        phrase_to_each_row = ['sartseq']\n        for index_word, word in enumerate(new_list_word):\n            if index_word == (len(new_list_word) - 1):\n                phrase_to_each_row.append(word)\n                list_to_csv.append([image, ' '.join(phrase_to_each_row), 'endseq'])\n            else:\n                phrase_to_each_row.append(word)\n                list_to_csv.append([image, ' '.join(phrase_to_each_row), new_list_word[index_word + 1]])","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:27.268106Z","iopub.execute_input":"2022-11-18T01:06:27.268437Z","iopub.status.idle":"2022-11-18T01:06:28.282587Z","shell.execute_reply.started":"2022-11-18T01:06:27.268406Z","shell.execute_reply":"2022-11-18T01:06:28.281992Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40460 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21784ac77f7342e8ab2d0b3edcfff954"}},"metadata":{}}]},{"cell_type":"code","source":"Ã­magens = []\nwith open('../input/flickr8k/Flickr8k_text/Flickr_8k.trainImages.txt', 'r') as train_images:\n    images = train_images.readlines()\n    imagens = [image.replace('.jpg\\n', '') for image in images]","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:28.283645Z","iopub.execute_input":"2022-11-18T01:06:28.284231Z","iopub.status.idle":"2022-11-18T01:06:28.292490Z","shell.execute_reply.started":"2022-11-18T01:06:28.284208Z","shell.execute_reply":"2022-11-18T01:06:28.291702Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data_frame_image_words = pd.DataFrame(list_to_csv, columns=['image', 'text_input', 'word_output'])\ndata_frame_image_words.to_csv('image_descriptions.csv', index=False)\ndata_frame_image_words.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:28.293810Z","iopub.execute_input":"2022-11-18T01:06:28.294035Z","iopub.status.idle":"2022-11-18T01:06:29.142343Z","shell.execute_reply.started":"2022-11-18T01:06:28.294012Z","shell.execute_reply":"2022-11-18T01:06:29.141375Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                   image                   text_input word_output\n0  1000268201_693b08cb0e                     startseq       child\n1  1000268201_693b08cb0e                sartseq child          in\n2  1000268201_693b08cb0e             sartseq child in        pink\n3  1000268201_693b08cb0e        sartseq child in pink       dress\n4  1000268201_693b08cb0e  sartseq child in pink dress          is","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>text_input</th>\n      <th>word_output</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e</td>\n      <td>startseq</td>\n      <td>child</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e</td>\n      <td>sartseq child</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e</td>\n      <td>sartseq child in</td>\n      <td>pink</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e</td>\n      <td>sartseq child in pink</td>\n      <td>dress</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e</td>\n      <td>sartseq child in pink dress</td>\n      <td>is</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data_frame = data_frame_image_words.loc[data_frame_image_words['image'].isin(imagens)].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:29.146194Z","iopub.execute_input":"2022-11-18T01:06:29.146438Z","iopub.status.idle":"2022-11-18T01:06:29.227135Z","shell.execute_reply.started":"2022-11-18T01:06:29.146417Z","shell.execute_reply":"2022-11-18T01:06:29.225270Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.nn.utils.rnn import pad_sequence\nimport random, os","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:29.230423Z","iopub.execute_input":"2022-11-18T01:06:29.230723Z","iopub.status.idle":"2022-11-18T01:06:30.763095Z","shell.execute_reply.started":"2022-11-18T01:06:29.230698Z","shell.execute_reply":"2022-11-18T01:06:30.761778Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'LR': 1e-3,\n    'EPOCHS': 5,\n    'BATCH_SIZE': 512,\n    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:07:00.414563Z","iopub.execute_input":"2022-11-18T01:07:00.414867Z","iopub.status.idle":"2022-11-18T01:07:00.419873Z","shell.execute_reply.started":"2022-11-18T01:07:00.414843Z","shell.execute_reply":"2022-11-18T01:07:00.418788Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class CustomDataLoader(torch.utils.data.Dataset):\n    def __init__(self, data_frame= None, word_to_index = None, images_dict = None):\n        self.data_frame = data_frame\n        self.images_dict = images_dict\n        self.word_to_index = word_to_index\n        self.pad_tensor = torch.ones(34)\n        self.num_classes = len(word_to_index)\n    \n    def __len__(self):\n        return len(self.data_frame)\n    \n    def to_categorical(self, y):\n    \n        return np.eye(self.num_classes + 1)[y]\n    \n    def __getitem__(self, index):\n        image_phrase_word = self.data_frame.iloc[index].values\n        \n        image_name = image_phrase_word[0]\n        phrase = image_phrase_word[1]\n        word_to_predict = image_phrase_word[2]\n        \n        phrase_to_index = [self.word_to_index[word] for word in phrase.split(' ') if word in self.word_to_index]\n        phrase_to_index = torch.FloatTensor(phrase_to_index)\n        \n        word_to_predict_index = self.word_to_index[word_to_predict]\n        \n        image = self.images_dict[image_name + '.jpg']\n        \n        image = torch.tensor(image, dtype=torch.float32)\n        phrase_indexs = torch.tensor(pad_sequence([phrase_to_index, self.pad_tensor], batch_first=True)[0], dtype=torch.long)\n        #target = torch.tensor(word_to_predict_index, dtype=torch.long)\n        target = torch.tensor(torch.from_numpy(self.to_categorical(word_to_predict_index)), dtype=torch.float32)\n\n        return image, phrase_indexs, target","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:30.772610Z","iopub.execute_input":"2022-11-18T01:06:30.772881Z","iopub.status.idle":"2022-11-18T01:06:30.785032Z","shell.execute_reply.started":"2022-11-18T01:06:30.772821Z","shell.execute_reply":"2022-11-18T01:06:30.784079Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train = CustomDataLoader(data_frame = train_data_frame, word_to_index = wordtoix, images_dict = train_features)\n\ntrain_loader = torch.utils.data.DataLoader(train,\n                                        shuffle=True,\n                                        pin_memory=True,\n                                        batch_size=CFG['BATCH_SIZE'],\n                                        num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:07:03.444139Z","iopub.execute_input":"2022-11-18T01:07:03.444462Z","iopub.status.idle":"2022-11-18T01:07:03.451176Z","shell.execute_reply.started":"2022-11-18T01:07:03.444438Z","shell.execute_reply":"2022-11-18T01:07:03.450023Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class CustomModel(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.dp_05 = nn.Dropout(p = 0.5)\n        self.dp_05_2 = nn.Dropout(p = 0.5)\n        self.linear_image_input = nn.Linear(2048, 256)\n        self.linear_output = nn.Linear(512, num_classes + 1)\n        self.embedding = nn.Embedding.from_pretrained(torch.from_numpy(embedding_matrix).type(torch.float32))\n        self.rnn = nn.LSTM(200, 256, batch_first=True)\n        self.relu = nn.ReLU()\n        \n    def forward(self, image_features, phrase):\n        image_features = self.dp_05(image_features)\n        image_features = self.linear_image_input(image_features)\n        image_features = self.relu(image_features)\n        \n        phrase_embedidng = self.embedding(phrase)\n        phrase_embedidng = self.dp_05_2(phrase_embedidng)\n        phrase_rnn = self.rnn(phrase_embedidng)[0]\n        image_features_and_phrase = torch.cat((image_features, phrase_rnn[:, -1, :]), 1)\n        output = self.linear_output(image_features_and_phrase)\n        \n        return output","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:30.802043Z","iopub.execute_input":"2022-11-18T01:06:30.803436Z","iopub.status.idle":"2022-11-18T01:06:30.813702Z","shell.execute_reply.started":"2022-11-18T01:06:30.803410Z","shell.execute_reply":"2022-11-18T01:06:30.812504Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = CustomModel(len(wordtoix)).to(CFG['DEVICE'])\n_loss = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=CFG['LR'])","metadata":{"execution":{"iopub.status.busy":"2022-11-18T01:06:30.815161Z","iopub.execute_input":"2022-11-18T01:06:30.815926Z","iopub.status.idle":"2022-11-18T01:06:30.846456Z","shell.execute_reply.started":"2022-11-18T01:06:30.815891Z","shell.execute_reply":"2022-11-18T01:06:30.845782Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"loss_mean = []\n\nsee_loss = 40\n\nfor epoch in range(CFG['EPOCHS']):\n    for i, (image, phrase, target) in enumerate(tqdm(train_loader, total=len(train_loader))):\n        \n        image = image.to(CFG['DEVICE'])\n        target = target.to(CFG['DEVICE'])\n        phrase = phrase.to(CFG['DEVICE'])\n        \n        output = model(image, phrase)\n        \n        optimizer.zero_grad()\n        loss = _loss(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        loss_mean.append(loss.item())\n        \n        if (i % see_loss) == 0:\n            print(np.mean(loss_mean))\n            loss_mean = []\n        \n        del image, phrase, target, output\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}